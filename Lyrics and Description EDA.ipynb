{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "## Stephen Kuc\n",
    "## 1/23/2023\n",
    "\n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df1e240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, downloading nltk\n",
    "\n",
    "# below is commented out code for this\n",
    "\n",
    "# import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\miniconda3\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/steph/OneDrive/Documents/USD/ADS509/Mod2/\"\n",
    "data_location_mine = \"/Users/steph/OneDrive/Documents/GitHub/ads509-api-scrape/\"\n",
    "\n",
    "# Going to utilize the sample data provided for this \n",
    "\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing descriptive_stats fucntion below\n",
    "\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        This function will do the following:\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # finding total number of tokens by the length of tokens list\n",
    "    num_tokens = len(tokens)\n",
    "    \n",
    "    # creating a set of tokens to find unique, and then length of that\n",
    "    num_unique_tokens = len(set(tokens))\n",
    "    \n",
    "    # dividing unique over total to find diversity\n",
    "    lexical_diversity = num_unique_tokens / num_tokens\n",
    "    \n",
    "    \n",
    "    # creating a quick for-loop to cound all characters in document\n",
    "    num_characters_list = []\n",
    "    for word in tokens:\n",
    "        num_characters_list.append(len(word))\n",
    "        \n",
    "    num_characters = sum(num_characters_list)\n",
    "    \n",
    "    # utilizing Counter to count the characters and then finding the top 5\n",
    "    \n",
    "    common_count = Counter(tokens)\n",
    "    \n",
    "    most_common_five = common_count.most_common(5)\n",
    "        \n",
    "    \n",
    "    if verbose == True:        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(f\"These are the five most common tokens and their count:\\n {most_common_five}\")\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters, most_common_five])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "These are the five most common tokens and their count:\n",
      " [('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "\n",
    "\n",
    "\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "### Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: Assertion statements in code essentially ensure that our code will run smoothly throughout the entire program, and when utilizing them when writing code to test the code, we can quickly debug. Aside from debugging and testing, it's a good way to document code during development too.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "039b6cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in lyric data\n",
    "# this for cher and Robyn data\n",
    "\n",
    "lyric_path = data_location + lyrics_folder\n",
    "\n",
    "lyric_dict_cr = {\"artist\": [], \"song_name\": [], \"lyrics\": []}\n",
    "\n",
    "for artist_folder in os.listdir(lyric_path):\n",
    "    artist_path = lyric_path + artist_folder\n",
    "    for file in os.listdir(artist_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = f\"{artist_path}/{file}\"\n",
    "            with open(file_path, 'r') as f:\n",
    "                lyrics = f.read()\n",
    "                song_name = file.split(\"_\")[1].split(\".\")[0] # take just song title from file name\n",
    "                \n",
    "                # adding data to dictionary\n",
    "                lyric_dict_cr['artist'].append(artist_folder)\n",
    "                lyric_dict_cr['song_name'].append(song_name)\n",
    "                lyric_dict_cr['lyrics'].append(lyrics)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf2a7d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in lyric data\n",
    "# this for Tom Petty and FT data\n",
    "\n",
    "lyric_path_ft = data_location_mine + lyrics_folder\n",
    "\n",
    "lyric_dict_ft = {\"artist\": [], \"song_name\": [], \"lyrics\": []}\n",
    "\n",
    "for artist_folder in os.listdir(lyric_path_ft):\n",
    "    artist_path = lyric_path_ft + artist_folder\n",
    "    for file in os.listdir(artist_path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = f\"{artist_path}/{file}\"\n",
    "            with open(file_path, 'r') as f:\n",
    "                lyrics = f.read()\n",
    "                song_name = file.split(\"_\")[1].split(\".\")[0] # take just song title from file name\n",
    "                \n",
    "                # adding data to dictionary\n",
    "                lyric_dict_ft['artist'].append(artist_folder)\n",
    "                lyric_dict_ft['song_name'].append(song_name)\n",
    "                lyric_dict_ft['lyrics'].append(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e87ce303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in twitter data, just for cher and robyn\n",
    "\n",
    "\n",
    "twit_path = data_location + twitter_folder\n",
    "\n",
    "twit_dict_cr = {\"artist\": [], \"description\": []}\n",
    "\n",
    "for file in os.listdir(twit_path):\n",
    "    if file.endswith('_data.txt'):\n",
    "        # saving artist name from folder name\n",
    "        artist = file.split('_')[0]\n",
    "        \n",
    "        # saving follower_path\n",
    "        file_path = twit_path + file\n",
    "        with open(file_path, 'r', encoding = \"utf8\") as f:\n",
    "            for line in f:\n",
    "                fields = line.strip().split(\"\\t\")\n",
    "                description = fields[-1] # description should be last field based on last assignment instrs.\n",
    "    \n",
    "                # adding data to dictionary\n",
    "                twit_dict_cr['artist'].append(artist)\n",
    "                twit_dict_cr['description'].append(description)\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632fb648",
   "metadata": {},
   "source": [
    "#### Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b15cf6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting by artist\n",
    "\n",
    "# below are my attempts at doing it within just a dictionary..\n",
    "\n",
    "twit_cher = {\"artist\": [], \"description\": []}\n",
    "twit_robyn = {\"artist\": [], \"description\": []}\n",
    "\n",
    "for key, item in twit_dict_cr.items():\n",
    "    if key == \"artist\" and item == \"cher\":\n",
    "        twit_cher.append(twit_dict_cr.get(\"artist\", \"description\"))\n",
    "    elif key == \"artist\" and item == \"robyn\":\n",
    "        twit_robyn['artist'].append(item)\n",
    "\n",
    "\n",
    "twit_cher = {\"artist\": \"cher\" for (key, value) in twit_dict_cr.items() if twit_dict_cr[\"artist\"] == \"cher\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63e542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick fix using dataframes and pandas\n",
    "\n",
    "twit_cr_df = pd.DataFrame(twit_dict_cr)\n",
    "\n",
    "twit_cher_df = twit_cr_df.loc[twit_cr_df['artist'] == 'cher']\n",
    "twit_robyn_df = twit_cr_df.loc[twit_cr_df['artist'] == 'robynkonichiwa']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c8fc547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuing with it\n",
    "twit_cher_df = twit_cher_df.astype(str)\n",
    "twit_robyn_df = twit_robyn_df.astype(str)\n",
    "\n",
    "twit_cher = twit_cher_df.to_dict()\n",
    "twit_robyn = twit_robyn_df.to_dict()\n",
    "\n",
    "dict_list = [twit_cher, twit_robyn]\n",
    "\n",
    "# need to ensure dictionaries are all strings\n",
    "\n",
    "for i in dict_list:\n",
    "    i['description'] = str(i['description'])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de3ed3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_robyn['description'] = str(twit_robyn['description'])\n",
    "twit_cher['description'] = str(twit_cher['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9e794",
   "metadata": {},
   "source": [
    "Can't quite get it figured out with just the dictionary; so, utilizing the dataframe created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cb032a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting on white space\n",
    "twit_cher_clean =  [txt.split(' ') for txt in twit_cher_df['description'].tolist()]\n",
    "twit_robyn_clean =  [txt.split(' ') for txt in twit_robyn_df['description'].tolist()]\n",
    "\n",
    "# that created a list of lists, need to flatten back to a 2-d list\n",
    "\n",
    "twit_cher_clean1 = [i for sublist in twit_cher_clean for i in sublist]\n",
    "twit_robyn_clean1 = [i for sublist in twit_robyn_clean for i in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53570cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folding to lower_case\n",
    "\n",
    "twit_cher_clean2 =  [txt.lower() for txt in twit_cher_clean1]\n",
    "\n",
    "twit_robyn_clean2 =  [txt.lower() for txt in twit_robyn_clean1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc847ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up function to remove stopwords\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def remove_stop(tokens):\n",
    "    return [t for t in tokens if t not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51b4bd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing function\n",
    "\n",
    "twit_cher_clean3 = remove_stop(twit_cher_clean2)\n",
    "twit_robyn_clean3 = remove_stop(twit_robyn_clean2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9d87a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using regular expression to filter punctuation\n",
    "\n",
    "import re \n",
    "r = re.compile(r'[^\\w\\s]+')\n",
    "               \n",
    "twit_cher_clean_fin = [r.sub('', s) for s in  twit_cher_clean3] \n",
    "twit_robyn_clean_fin = [r.sub('', s) for s in  twit_robyn_clean3] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation using translate and make trans\n",
    "\n",
    "# within list comprehension for loop\n",
    "# using punctuation from string on import\n",
    "\n",
    "# twit_cher_clean = [txt.translate(txt.maketrans('', '', punctuation)) for txt in twit_cher_df['description']]\n",
    "# twit_robyn_clean = [txt.translate(txt.maketrans('', '', punctuation)) for txt in twit_robyn_df['description']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "128b240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_cher_corpus = ' '.join(twit_cher_clean_fin)\n",
    "twit_robyn_corpus = ' '.join(twit_robyn_clean_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8af6d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "twit_cher_tokens =word_tokenize(twit_cher_corpus)\n",
    "twit_robyn_tokens = word_tokenize(twit_robyn_corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32926cac",
   "metadata": {},
   "source": [
    "### Cleaning Lyric Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08b357",
   "metadata": {},
   "source": [
    "#### Cher and Robyn Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e21202a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting by artist using dataframes\n",
    "\n",
    "lyric_df_cr = pd.DataFrame(lyric_dict_cr)\n",
    "\n",
    "lyric_cher_df = lyric_df_cr.loc[lyric_df_cr['artist'] == 'cher']\n",
    "lyric_robyn_df = lyric_df_cr.loc[lyric_df_cr['artist'] == 'robyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d65056e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of \\n that are stuck in text\n",
    "\n",
    "lyric_cher_clean =  [txt.replace('\\n',' ') for txt in lyric_cher_df['lyrics']]\n",
    "lyric_robyn_clean =  [txt.replace('\\n',' ')  for txt in lyric_robyn_df['lyrics']]\n",
    "\n",
    "# splitting on white space\n",
    "lyric_cher_clean1 =  [txt.split(' ') for txt in lyric_cher_clean]\n",
    "lyric_robyn_clean1 =  [txt.split(' ') for txt in lyric_robyn_clean]\n",
    "\n",
    "# that created a list of lists, need to flatten back to a 2-d list\n",
    "\n",
    "lyric_cher_clean1 = [i for sublist in lyric_cher_clean1 for i in sublist]\n",
    "lyric_robyn_clean1 = [i for sublist in lyric_robyn_clean1 for i in sublist]\n",
    "\n",
    "# folding to lower_case\n",
    "\n",
    "lyric_cher_clean2 =  [txt.lower() for txt in lyric_cher_clean1]\n",
    "\n",
    "lyric_robyn_clean2 =  [txt.lower() for txt in lyric_robyn_clean1]\n",
    "\n",
    "# getting rid of blank entries\n",
    "while \"\" in lyric_cher_clean2:\n",
    "    lyric_cher_clean2.remove(\"\")\n",
    "    \n",
    "    \n",
    "while \"\" in lyric_robyn_clean2:\n",
    "    lyric_robyn_clean2.remove(\"\")\n",
    "    \n",
    "# remove stopwords\n",
    "\n",
    "lyric_cher_clean3 = remove_stop(lyric_cher_clean2)\n",
    "lyric_robyn_clean3 = remove_stop(lyric_robyn_clean2)\n",
    "\n",
    "# removing punctuation \n",
    "# using r as set in twitter step\n",
    "\n",
    "lyric_cher_clean_fin  = [r.sub('', s) for s in  lyric_cher_clean3] \n",
    "lyric_robyn_clean_fin = [r.sub('', s) for s in  lyric_robyn_clean3] \n",
    "\n",
    "\n",
    "# finalizing tokens\n",
    "\n",
    "lyric_cher_corpus = ' '.join(lyric_cher_clean_fin)\n",
    "lyric_robyn_corpus = ' '.join(lyric_robyn_clean_fin)\n",
    "\n",
    "lyric_cher_tokens = word_tokenize(lyric_cher_corpus)\n",
    "lyric_robyn_tokens = word_tokenize(lyric_robyn_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f831bc",
   "metadata": {},
   "source": [
    "#### tom petty and frank turner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08b278c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting by artist using dataframes\n",
    "\n",
    "lyric_df_ft = pd.DataFrame(lyric_dict_ft)\n",
    "\n",
    "lyric_tp_df = lyric_df_ft.loc[lyric_df_ft['artist'] == 'tompetty']\n",
    "lyric_ft_df = lyric_df_ft.loc[lyric_df_ft['artist'] == 'frankturner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e4bc87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting rid of \\n that are stuck in text\n",
    "\n",
    "lyric_tp_clean =  [txt.replace('\\\\n',' ')  for txt in lyric_tp_df['lyrics']]\n",
    "lyric_ft_clean =  [txt.replace('\\\\n',' ')  for txt in lyric_ft_df['lyrics']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a43ee2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting on white space\n",
    "lyric_tp_clean1 =  [txt.split(' ') for txt in lyric_tp_clean]\n",
    "lyric_ft_clean1 =  [txt.split(' ') for txt in lyric_ft_clean]\n",
    "\n",
    "# that created a list of lists, need to flatten back to a 2-d list\n",
    "\n",
    "lyric_tp_clean1 = [i for sublist in lyric_tp_clean1 for i in sublist]\n",
    "lyric_ft_clean1 = [i for sublist in lyric_ft_clean1 for i in sublist]\n",
    "\n",
    "# folding to lower_case\n",
    "\n",
    "lyric_tp_clean2 =  [txt.lower() for txt in lyric_tp_clean1]\n",
    "\n",
    "lyric_ft_clean2 =  [txt.lower() for txt in lyric_ft_clean1]\n",
    "\n",
    "# getting rid of blank entries\n",
    "while \"\" in lyric_tp_clean2:\n",
    "    lyric_tp_clean2.remove(\"\")\n",
    "    \n",
    "    \n",
    "while \"\" in lyric_ft_clean2:\n",
    "    lyric_ft_clean2.remove(\"\")\n",
    "    \n",
    "# putting in an apostrophe for u2019 in ft lyrics\n",
    "    \n",
    "lyric_ft_clean2 = [re.sub(\"u2019\", \"'\", s) for s in lyric_ft_clean2]\n",
    "\n",
    "# and a dash for u2013\n",
    "lyric_ft_clean2 = [re.sub(\"u2013\", \"-\", s) for s in lyric_ft_clean2]\n",
    "    \n",
    "# remove stopwords\n",
    "\n",
    "lyric_tp_clean3 = remove_stop(lyric_tp_clean2)\n",
    "lyric_ft_clean3 = remove_stop(lyric_ft_clean2)\n",
    "\n",
    "\n",
    "# removing punctuation \n",
    "# using r as set in twitter step\n",
    "\n",
    "lyric_tp_clean_fin  = [r.sub('', s) for s in  lyric_tp_clean3] \n",
    "lyric_ft_clean_fin = [r.sub('', s) for s in  lyric_ft_clean3] \n",
    "\n",
    "\n",
    "# finalizing tokens\n",
    "\n",
    "lyric_tp_corpus = ' '.join(lyric_tp_clean_fin)\n",
    "lyric_ft_corpus = ' '.join(lyric_ft_clean_fin)\n",
    "\n",
    "lyric_tp_tokens = word_tokenize(lyric_tp_corpus)\n",
    "lyric_ft_tokens = word_tokenize(lyric_ft_corpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a99f0",
   "metadata": {},
   "source": [
    "# Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36201 tokens in the data.\n",
      "There are 3779 unique tokens in the data.\n",
      "There are 169727 characters in the data.\n",
      "The lexical diversity is 0.104 in the data.\n",
      "These are the five most common tokens and their count:\n",
      " [('love', 1004), ('im', 513), ('know', 486), ('got', 352), ('na', 351)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[36201,\n",
       " 3779,\n",
       " 0.1043893815087981,\n",
       " 169727,\n",
       " [('love', 1004), ('im', 513), ('know', 486), ('got', 352), ('na', 351)]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls to descriptive_stats here\n",
    "\n",
    "# cher lyrics\n",
    "\n",
    "descriptive_stats(lyric_cher_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "595fb01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 15477 tokens in the data.\n",
      "There are 2214 unique tokens in the data.\n",
      "There are 73109 characters in the data.\n",
      "The lexical diversity is 0.143 in the data.\n",
      "These are the five most common tokens and their count:\n",
      " [('know', 308), ('im', 299), ('got', 276), ('love', 275), ('like', 232)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[15477,\n",
       " 2214,\n",
       " 0.1430509788718744,\n",
       " 73109,\n",
       " [('know', 308), ('im', 299), ('got', 276), ('love', 275), ('like', 232)]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# robyn lyrics\n",
    "\n",
    "descriptive_stats(lyric_robyn_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cfb90cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2055 tokens in the data.\n",
      "There are 534 unique tokens in the data.\n",
      "There are 9844 characters in the data.\n",
      "The lexical diversity is 0.260 in the data.\n",
      "These are the five most common tokens and their count:\n",
      " [('know', 80), ('need', 50), ('baby', 47), ('like', 43), ('yeah', 39)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2055,\n",
       " 534,\n",
       " 0.25985401459854013,\n",
       " 9844,\n",
       " [('know', 80), ('need', 50), ('baby', 47), ('like', 43), ('yeah', 39)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tom petty lyrics\n",
    "\n",
    "descriptive_stats(lyric_tp_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "948880c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3110 tokens in the data.\n",
      "There are 1253 unique tokens in the data.\n",
      "There are 16010 characters in the data.\n",
      "The lexical diversity is 0.403 in the data.\n",
      "These are the five most common tokens and their count:\n",
      " [('im', 49), ('get', 47), ('never', 26), ('got', 25), ('well', 23)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3110,\n",
       " 1253,\n",
       " 0.4028938906752412,\n",
       " 16010,\n",
       " [('im', 49), ('get', 47), ('never', 26), ('got', 25), ('well', 23)]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frank turner lyrics\n",
    "\n",
    "descriptive_stats(lyric_ft_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08074edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 17343902 tokens in the data.\n",
      "There are 1236291 unique tokens in the data.\n",
      "There are 96714175 characters in the data.\n",
      "The lexical diversity is 0.071 in the data.\n",
      "These are the five most common tokens and their count:\n",
      " [('love', 216699), ('im', 163268), ('life', 125498), ('music', 89472), ('de', 73059)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[17343902,\n",
       " 1236291,\n",
       " 0.07128101853896546,\n",
       " 96714175,\n",
       " [('love', 216699),\n",
       "  ('im', 163268),\n",
       "  ('life', 125498),\n",
       "  ('music', 89472),\n",
       "  ('de', 73059)]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cher twitter descriptions\n",
    "\n",
    "descriptive_stats(twit_cher_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eaf0dd02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1638560 tokens in the data.\n",
      "There are 232325 unique tokens in the data.\n",
      "There are 9443296 characters in the data.\n",
      "The lexical diversity is 0.142 in the data.\n",
      "These are the five most common tokens and their count:\n",
      " [('music', 15287), ('love', 11791), ('im', 10376), ('och', 7923), ('life', 7518)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1638560,\n",
       " 232325,\n",
       " 0.1417860804608925,\n",
       " 9443296,\n",
       " [('music', 15287),\n",
       "  ('love', 11791),\n",
       "  ('im', 10376),\n",
       "  ('och', 7923),\n",
       "  ('life', 7518)]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# robyn twitter description\n",
    "\n",
    "descriptive_stats(twit_robyn_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde78a11",
   "metadata": {},
   "source": [
    "#### Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: I'd imagine they would be much more generic words that can be used in nearly any context, thus giving us very little tangible information from the corpus. Taking a glance at some of the stop words, some words that seem could be very high in any situation could be like \"and\", \"am\", \"because\", \"it's\", 'i', and others. Although, it seemed some \"I'm\"s slipped through in our pre-processing. \n",
    "\n",
    "It's important to note, as well, that the words in the stopwords set contained punctuation like \"it's\" or \"don't\". In the above steps, I removed the punctuation after removing stopwords.\n",
    "\n",
    "---\n",
    "\n",
    "#### Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: I can really only answer this for the two artists I chose, Tom Petty and Frank Turner. I'd say that the difference of lexical diversity does about conform to my prior beliefs. Tom's songs are relatively simple and to the point, and Frank Turner has some lyric heavy songs with often-times some interesting references or usage of words. So, I expected Frank to have a higher lexical diversity, and it appeared to be true with his .406 vs Tom's .260. Still, Tom's is much higher than both Robyn and Cher's. I'm not very familiar with Robyn, but it also makes sense that there would be less diversity with pop-like music. \n",
    "\n",
    "Also, Robyn and Cher each have a much larger dataset we're working off of, which I'd hypothesize would drive down the diversity naturally. I only looked at the first 20 songs for each Frank and Tom's discography, due to the issues with scraping data last assignment, and these are chronological as well. I'd like to scrap the rest and really see how, perhaps, their careers progressed with the uniqueness of words used. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"❤️\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a5974",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(emoji)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis 😁\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "806f65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting emojis\n",
    "\n",
    "import emoji\n",
    "\n",
    "def extract_emojis(s):\n",
    "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI['en'])\n",
    "\n",
    "cher_emoji = extract_emojis(twit_cher_df['description'])\n",
    "\n",
    "robyn_emoji = extract_emojis(twit_robyn_df['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ae18063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some quick cleanup\n",
    "\n",
    "# getting rid of some unicode leftover\n",
    "\n",
    "cher_emoji1 = [re.sub(\"\\u200d\", \"\", s) for s in cher_emoji]\n",
    "\n",
    "robyn_emoji1 = [re.sub(\"\\u200d\", \"\", s) for s in robyn_emoji]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c1d588",
   "metadata": {},
   "source": [
    "Tried below cells to clean blank list elements. no method seems to be working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9dbd8445",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_emoji2 = ' '.join(cher_emoji1).split(' ')\n",
    "    \n",
    "while '' in robyn_emoji1:\n",
    "    robyn_emoji1.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8b515b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in robyn_emoji1:\n",
    "    if (i == ''):\n",
    "        robyn_emoji1.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0f4fcfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_emoji2 = ' '.join(cher_emoji1)\n",
    "robyn_emoji2 = ' '.join(robyn_emoji1) \n",
    "\n",
    "cher_emoji2 = re.findall(r'.', cher_emoji2)\n",
    "robyn_emoji2 = re.findall(r'.', robyn_emoji2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "269cd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "# utilizing Counter to count the characters and then finding the top 5\n",
    "    \n",
    "common_count_cher = Counter(cher_emoji2)\n",
    "common_count_robyn = Counter(robyn_emoji2)\n",
    "\n",
    "most_common_ten_cher = common_count_cher.most_common(12)  ## using twelve in case there are any unicode or '' that get in\n",
    "most_common_ten_robyn = common_count_robyn.most_common(12) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c0349937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are Cher's followers' 10 most common emojis used:  [(' ', 21058), ('️', 2322), ('❤', 866), ('🌈', 627), ('✌', 529), ('🏻', 456), ('🏳', 454), ('♀', 419), ('✨', 309), ('🏼', 293), ('🇦', 289), ('🤷', 261)]\n",
      "Here are Robyn's followers' 10 most common emojis used:  [(' ', 1708), ('️', 198), ('❤', 77), ('✌', 57), ('🌈', 50), ('🏳', 38), ('🏼', 35), ('♀', 32), ('♥', 32), ('🏻', 31), ('✨', 30), ('🤷', 28)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are Cher's followers' 10 most common emojis used: \", most_common_ten_cher)\n",
    "\n",
    "print(\"Here are Robyn's followers' 10 most common emojis used: \", most_common_ten_robyn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8606942",
   "metadata": {},
   "source": [
    "There were some similarities between each artist followers for their most used emojis. The mid-sized black heart was each top emoji; they each had at least 2 black hearts in the top 10. There was a blue block of a certain letter for each. The rainbow, female gender symbol, shrugging emoji, white flags, and stars seemed to also be an overlap between each. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "07c396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "cher_ht = ' '.join(twit_cher_clean3)\n",
    "robyn_ht = ' '.join(twit_robyn_clean3) \n",
    "\n",
    "\n",
    "cher_ht = re.findall(r'#\\b\\w+',cher_ht)\n",
    "robyn_ht = re.findall(r'#\\b\\w+',robyn_ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3877bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizing Counter to count the characters and then finding the top 5\n",
    "    \n",
    "ht_count_cher = Counter(cher_ht)\n",
    "ht_count_robyn = Counter(robyn_ht)\n",
    "\n",
    "most_common_ht_cher = ht_count_cher.most_common(10) \n",
    "most_common_ht_robyn = ht_count_robyn.most_common(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "1c475c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are Cher's followers' 10 most common hashtags used:  [('#resist', 11657), ('#blm', 10466), ('#blacklivesmatter', 8154), ('#theresistance', 3508), ('#fbr', 3419), ('#resistance', 3061), ('#1', 2627), ('#voteblue', 2304), ('#lgbtq', 2074), ('#music', 1602)]\n",
      "Here are Robyn's followers' 10 most common hashtags used:  [('#blacklivesmatter', 601), ('#blm', 362), ('#music', 306), ('#1', 199), ('#teamfollowback', 135), ('#edm', 111), ('#lgbtq', 89), ('#resist', 86), ('#travel', 73), ('#art', 72)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are Cher's followers' 10 most common hashtags used: \", most_common_ht_cher)\n",
    "\n",
    "print(\"Here are Robyn's followers' 10 most common hashtags used: \", most_common_ht_robyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21625593",
   "metadata": {},
   "source": [
    "There are some common ones among these like resist, blm, blacklivesmatter, lgbtq, and music. Perhaps there are opportunities to further clean this and bucket some into broader hashtag cateogries, like blm and black lives matter into one; or resist and resistance into one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i will go back to the petty and frank turner data\n",
    "tp_lyr = list(lyric_tp_df['lyrics'])\n",
    "\n",
    "ft_lyr = list(lyric_ft_df['lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "5a34e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_lyrs = []\n",
    "ft_lyrs = []\n",
    "\n",
    "for item in tp_lyr:\n",
    "    tp_lyrs.append(item.split(\"|\"))\n",
    "for item in ft_lyr:\n",
    "    ft_lyrs.append(item.split(\"|\"))\n",
    "    \n",
    "tp_lyrs = [i for sublist in tp_lyrs for i in sublist]\n",
    "ft_lyrs = [i for sublist in ft_lyrs for i in sublist]\n",
    "\n",
    "# these are now split with every other element being the song title\n",
    "\n",
    "tp_names = tp_lyrs[::2] # every 2\n",
    "ft_names = ft_lyrs[::2]\n",
    "\n",
    "# now to get rid of artist name \n",
    "\n",
    "tp_names_s = [re.sub(r\"Tom\\sPetty\\sAnd\\sThe\\sHeartbreakers\\s-\\s\", \"\", s) for s in tp_names]\n",
    "ft_names_s = [re.sub(r\"Frank\\sTurner\\s-\\s\", \"\", s) for s in ft_names]\n",
    "\n",
    "# get rid of 'lyrics' in name\n",
    "\n",
    "tp_names_s = [re.sub(r\"\\sLyrics\\s\", \"\", s) for s in tp_names_s]\n",
    "ft_names_s = [re.sub(r\"\\sLyrics\\s\", \"\", s) for s in ft_names_s]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "014a1567",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_names_s_all = ' '.join(tp_names_s)\n",
    "ft_names_s_all = ' '.join(ft_names_s) \n",
    "\n",
    "# sw for song word\n",
    "tp_sw = re.findall(r'\\b\\w+\\'*\\w+',tp_names_s_all)\n",
    "ft_sw = re.findall(r'\\b\\w+\\'*\\w+',ft_names_s_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "637228d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizing Counter to count the characters and then finding the top 5\n",
    "    \n",
    "tp_count_sw = Counter(tp_sw)\n",
    "ft_count_sw = Counter(ft_sw)\n",
    "\n",
    "most_common_sw_tp = tp_count_sw.most_common(10) \n",
    "most_common_sw_ft  = ft_count_sw.most_common(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6427fdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are Tom Petty's most common words in song titles (for the 20 we looked at):  [('The', 3), ('Rock', 2), ('It', 2), ('To', 2), ('American', 1), ('Girl', 1), ('Anything', 1), (\"That's\", 1), ('Roll', 1), (\"Baby's\", 1)]\n",
      "Here are Frank Turner's most common words in song titles (for the 20 we looked at):  [('The', 4), ('Of', 3), ('Day', 2), ('My', 2), ('To', 2), ('Decent', 1), ('Cup', 1), ('Tea', 1), ('Back', 1), ('In', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Here are Tom Petty's most common words in song titles (for the 20 we looked at): \", most_common_sw_tp)\n",
    "\n",
    "print(\"Here are Frank Turner's most common words in song titles (for the 20 we looked at): \", most_common_sw_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de3d02b",
   "metadata": {},
   "source": [
    "The are definitely some stopwords in here, but I'd argue that song titles, since they're so short, each word can hold some relevance. Also, I'd  of course like to look at the larger, full corpus of all their song names. I need to load more from that first assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "#### Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: This matches on any whitespace character (\\s) one or more times (+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "e0faa09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding length for each song\n",
    "\n",
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])\n",
    "\n",
    "tp_lengths = []\n",
    "ft_lengths = []\n",
    "\n",
    "for i in tp_lyr:\n",
    "    tokens = tokenize_lyrics(i)\n",
    "    tp_lengths.append(len(tokens))\n",
    "for i in ft_lyr:\n",
    "    tokens = tokenize_lyrics(i)\n",
    "    ft_lengths.append(len(tokens))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e71ec3",
   "metadata": {},
   "source": [
    "### lyric length comparison chart is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "805a1e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "Frank Turner    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Tom Petty       AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: length, dtype: object"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_replicates = 1000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"artist\" : ['Tom Petty'] * len(tp_lengths) + ['Frank Turner']*len(ft_lengths),\n",
    "    \"length\" : np.concatenate((tp_song_length['length'],ft_song_length['length']))})\n",
    "\n",
    "df.groupby('artist')['length'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
